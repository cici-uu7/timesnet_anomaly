"""
TimesNetPro: TimesNet with Adaptive Period Attention

Ê†∏ÂøÉÊîπËøõÔºöËá™ÈÄÇÂ∫îÂë®ÊúüÊ≥®ÊÑèÂäõ (Adaptive Period Attention)

ÈóÆÈ¢òÂàÜÊûêÔºö
- ÂéüÁâà TimesNet ‰ΩøÁî®ÁÆÄÂçïÁöÑ softmax(FFTÂπÖÂ∫¶) ‰Ωú‰∏∫Âë®ÊúüÊùÉÈáç
- ÂÅáËÆæÊâÄÊúâ Top-k Âë®ÊúüÂêåÁ≠âÈáçË¶ÅÔºåÁõ¥Êé•Áõ∏Âä†‰ºö"Á®ÄÈáä"ÂºÇÂ∏∏‰ø°Âè∑
- ÂºÇÂ∏∏ÂæÄÂæÄÂè™Á†¥ÂùèÊüê‰∏Ä‰∏™ÁâπÂÆöÂë®ÊúüÔºåÂÖ∂‰ªñÂë®ÊúüÂèØËÉΩÊ≠£Â∏∏

Ëß£ÂÜ≥ÊñπÊ°àÔºö
- ÂºïÂÖ•ÂèØÂ≠¶‰π†ÁöÑÊ≥®ÊÑèÂäõÊú∫Âà∂ÔºåËÆ©Ê®°ÂûãÂä®ÊÄÅÂà§Êñ≠ÊØè‰∏™Âë®ÊúüÁöÑ"‰ø°ÊÅØÈáè"
- ÁªìÂêà FFT ÊùÉÈáçÔºàÈ¢ëÂüüÂÖàÈ™åÔºâÂíåÂ≠¶‰π†ÁöÑÊ≥®ÊÑèÂäõÊùÉÈáçÔºàÊï∞ÊçÆÈ©±Âä®Ôºâ
- ‰ΩøÁî®ËΩªÈáèÁ∫ßÁöÑÊ≥®ÊÑèÂäõÊ®°ÂùóÔºå‰∏çÂ¢ûÂä†Â§™Â§öËÆ°ÁÆóÂºÄÈîÄ

‰ºòÂäøÔºö
- ÊîπÂä®ÊúÄÂ∞èÔºåÈÄªËæëÊúÄÁ°¨
- ‰øùÊåÅ TimesNet ÁöÑÊ†∏ÂøÉÊû∂ÊûÑ‰∏çÂèò
- ÈÄÇÁî®‰∫éÂºÇÂ∏∏Ê£ÄÊµã‰ªªÂä°ÔºåËÉΩÊõ¥Â•ΩÂú∞ÊçïÊçâÂë®ÊúüÂºÇÂ∏∏
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from models.TimesNet import FFT_for_Period
from layers.Conv_Blocks import Inception_Block_V1


class AdaptivePeriodAttention(nn.Module):
    """
    Ëá™ÈÄÇÂ∫îÂë®ÊúüÊ≥®ÊÑèÂäõÊ®°Âùó
    
    ÂØπÊØè‰∏™Âë®ÊúüÁöÑÁâπÂæÅËøõË°åÁºñÁ†ÅÔºåÂ≠¶‰π†ÂÖ∂ÈáçË¶ÅÊÄßÊùÉÈáç
    ÁªìÂêà FFT ÊùÉÈáçÔºàÈ¢ëÂüüÂÖàÈ™åÔºâÂíåÂ≠¶‰π†ÁöÑÊ≥®ÊÑèÂäõÊùÉÈáç
    """
    def __init__(self, d_model, d_attn=None, dropout=0.1):
        """
        Args:
            d_model: ÁâπÂæÅÁª¥Â∫¶
            d_attn: Ê≥®ÊÑèÂäõÈöêËóèÁª¥Â∫¶ÔºàÈªòËÆ§ d_model // 4Ôºâ
            dropout: Dropout ÊØîÁéá
        """
        super(AdaptivePeriodAttention, self).__init__()
        self.d_model = d_model
        self.d_attn = d_attn or (d_model // 4)
        
        # Âë®ÊúüÁâπÂæÅÁºñÁ†ÅÂô®ÔºöÂ∞ÜÊØè‰∏™Âë®ÊúüÁöÑÁâπÂæÅÁºñÁ†Å‰∏∫Âõ∫ÂÆöÁª¥Â∫¶
        # ‰ΩøÁî®ÂÖ®Â±ÄÊ±†Âåñ + MLP Êù•ÊèêÂèñÂë®ÊúüÁ∫ßÂà´ÁöÑÁâπÂæÅ
        self.period_encoder = nn.Sequential(
            nn.Linear(d_model, self.d_attn),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(self.d_attn, self.d_attn)
        )
        
        # Query ÁîüÊàêÂô®Ôºö‰ªéËæìÂÖ•Â∫èÂàóÁîüÊàêÊü•ËØ¢ÂêëÈáè
        # Áî®‰∫éÂà§Êñ≠Âì™‰∫õÂë®ÊúüÂØπÂΩìÂâçÊ†∑Êú¨Êõ¥ÈáçË¶Å
        self.query_proj = nn.Linear(d_model, self.d_attn)
        
        # Ê∏©Â∫¶ÂèÇÊï∞ÔºàÂèØÂ≠¶‰π†ÔºâÔºåÁî®‰∫éÊéßÂà∂Ê≥®ÊÑèÂäõÂàÜÂ∏ÉÁöÑÈîêÂ∫¶
        self.temperature = nn.Parameter(torch.ones(1))
        
        # ËûçÂêàÊùÉÈáçÔºöÂπ≥Ë°° FFT ÊùÉÈáçÂíåÂ≠¶‰π†ÁöÑÊ≥®ÊÑèÂäõÊùÉÈáç
        self.alpha = nn.Parameter(torch.tensor(0.5))  # ÂàùÂßãÂÄºÔºö‰∏§ËÄÖÂêÑÂç†‰∏ÄÂçä
        
        # Ê≥®ÊÑèÔºöÂú® TimesNet ‰∏≠ÔºåËæìÂÖ•ÁâπÂæÅÁª¥Â∫¶ N ÈÄöÂ∏∏Á≠â‰∫é d_model
        # ÊâÄ‰ª• period_global ÁöÑÁª¥Â∫¶Â∫îËØ•ÊòØ [B, K, d_model]Ôºå‰∏çÈúÄË¶ÅÈ¢ùÂ§ñÊäïÂΩ±
        
    def forward(self, period_features, fft_weights, x_input):
        """
        Args:
            period_features: [B, T, N, K] K ‰∏™Âë®ÊúüÁöÑÁâπÂæÅ
            fft_weights: [B, K] FFT ËÆ°ÁÆóÁöÑÂéüÂßãÊùÉÈáç
            x_input: [B, T, N] ÂéüÂßãËæìÂÖ•ÔºåÁî®‰∫éÁîüÊàê query
        
        Returns:
            attention_weights: [B, T, N, K] ÊúÄÁªàÁöÑÂë®ÊúüÊ≥®ÊÑèÂäõÊùÉÈáç
        """
        B, T, N, K = period_features.shape
        
        # 1. ÂØπÊØè‰∏™Âë®ÊúüÁöÑÁâπÂæÅËøõË°åÁºñÁ†Å
        # period_features: [B, T, N, K] -> [B, K, T, N]
        period_feat = period_features.permute(0, 3, 1, 2).contiguous()  # [B, K, T, N]
        
        # ÂÖ®Â±ÄÊ±†ÂåñÔºöÊèêÂèñÊØè‰∏™Âë®ÊúüÁöÑÂÖ®Â±ÄÁâπÂæÅ
        # ÂØπÊØè‰∏™Âë®ÊúüÔºåÂú®Êó∂Èó¥Áª¥Â∫¶‰∏äËøõË°åÂÖ®Â±ÄÂπ≥ÂùáÊ±†Âåñ
        # [B, K, T, N] -> [B, K, N] (Âú® TimesNet ‰∏≠ÔºåN == d_model)
        period_global = period_feat.mean(dim=2)  # [B, K, N] ÂØπÊó∂Èó¥Áª¥Â∫¶Ê±ÇÂπ≥Âùá
        
        # ÁºñÁ†ÅÂë®ÊúüÁâπÂæÅ: [B, K, d_model] -> [B, K, d_attn]
        period_encoded = self.period_encoder(period_global)  # [B, K, d_attn]
        
        # 2. ‰ªéËæìÂÖ•ÁîüÊàê Query
        # ‰ΩøÁî®ËæìÂÖ•Â∫èÂàóÁöÑÂÖ®Â±ÄÁâπÂæÅ‰Ωú‰∏∫ query
        x_global = x_input.mean(dim=1)  # [B, N] -> ÂØπÊó∂Èó¥Áª¥Â∫¶Ê±ÇÂπ≥Âùá
        query = self.query_proj(x_global)  # [B, d_attn]
        query = query.unsqueeze(1)  # [B, 1, d_attn]
        
        # 3. ËÆ°ÁÆóÊ≥®ÊÑèÂäõÂàÜÊï∞
        # query: [B, 1, d_attn], period_encoded: [B, K, d_attn]
        attn_scores = torch.matmul(query, period_encoded.transpose(1, 2))  # [B, 1, K]
        attn_scores = attn_scores / (self.temperature + 1e-8)  # Ê∏©Â∫¶Áº©Êîæ
        attn_scores = attn_scores.squeeze(1)  # [B, K]
        
        # 4. ÁªìÂêà FFT ÊùÉÈáçÂíåÂ≠¶‰π†ÁöÑÊ≥®ÊÑèÂäõÊùÉÈáç
        # FFT ÊùÉÈáçÂΩí‰∏ÄÂåñ
        fft_weights_norm = F.softmax(fft_weights, dim=1)  # [B, K]
        
        # Â≠¶‰π†ÁöÑÊ≥®ÊÑèÂäõÊùÉÈáçÂΩí‰∏ÄÂåñ
        learned_weights = F.softmax(attn_scores, dim=1)  # [B, K]
        
        # ËûçÂêàÔºöalpha ÊéßÂà∂‰∏§ËÄÖÁöÑÂπ≥Ë°°ÔºàÂèØÂ≠¶‰π†Ôºâ
        alpha = torch.sigmoid(self.alpha)  # Á°Æ‰øùÂú® [0, 1] ‰πãÈó¥
        fused_weights = alpha * learned_weights + (1 - alpha) * fft_weights_norm  # [B, K]
        
        # 5. Êâ©Â±ïÂà∞ [B, T, N, K] Áª¥Â∫¶
        fused_weights = fused_weights.unsqueeze(1).unsqueeze(2)  # [B, 1, 1, K]
        attention_weights = fused_weights.repeat(1, T, N, 1)  # [B, T, N, K]
        
        return attention_weights


class TimesBlockPro(nn.Module):
    """
    TimesBlock with Adaptive Period Attention
    
    ÊîπËøõÁÇπÔºö
    1. ‰ΩøÁî®Ëá™ÈÄÇÂ∫îÂë®ÊúüÊ≥®ÊÑèÂäõÊõø‰ª£ÁÆÄÂçïÁöÑ FFT ÊùÉÈáçËÅöÂêà
    2. ‰øùÊåÅ TimesNet ÁöÑÊ†∏ÂøÉÊû∂ÊûÑÔºà2D ConvÔºâ‰∏çÂèò
    3. ÂèØÂ≠¶‰π†ÁöÑÊ≥®ÊÑèÂäõÊú∫Âà∂ËÉΩÊõ¥Â•ΩÂú∞ÊçïÊçâÂë®ÊúüÂºÇÂ∏∏
    """
    def __init__(self, configs):
        super(TimesBlockPro, self).__init__()
        self.seq_len = configs.seq_len
        self.pred_len = configs.pred_len
        self.k = configs.top_k
        self.d_model = configs.d_model
        
        # 2D Âç∑ÁßØÂùóÔºà‰∏éÂéüÁâà‰∏ÄËá¥Ôºâ
        self.conv = nn.Sequential(
            Inception_Block_V1(configs.d_model, configs.d_ff,
                               num_kernels=configs.num_kernels),
            nn.GELU(),
            Inception_Block_V1(configs.d_ff, configs.d_model,
                               num_kernels=configs.num_kernels)
        )
        
        # Ëá™ÈÄÇÂ∫îÂë®ÊúüÊ≥®ÊÑèÂäõÊ®°Âùó
        dropout_rate = getattr(configs, 'dropout', 0.1)
        self.period_attention = AdaptivePeriodAttention(
            d_model=configs.d_model,
            dropout=dropout_rate
        )
        
    def forward(self, x):
        """
        Args:
            x: [B, T, N] ËæìÂÖ•Â∫èÂàó
        
        Returns:
            res: [B, T, N] ËæìÂá∫ÁâπÂæÅ
        """
        B, T, N = x.size()

        # ============================================================
        # üß™ Ê∂àËûçÂÆûÈ™åÊ®°ÂºèÔºöÂº∫Âà∂ÈÄÄÂåñÂõûÂéüÁâà TimesNet ÈÄªËæë
        # ‰ªÖ‰ΩøÁî® FFT ÂπÖÂ∫¶ period_weight ‰Ωú‰∏∫Âë®ÊúüËÅöÂêàÊùÉÈáç
        # ============================================================

        # 1. FFT ÊèêÂèñÂë®ÊúüÂíåÂéüÂßãÊùÉÈáçÔºàÂπÖÂ∫¶Ôºâ
        period_list, period_weight = FFT_for_Period(x, self.k)

        # 2. ÂØπÊØè‰∏™Âë®ÊúüËøõË°å 2D Âç∑ÁßØÂ§ÑÁêÜÔºà‰∏éÂéüÁâà TimesBlock ÂÆåÂÖ®‰∏ÄËá¥Ôºâ
        res = []
        for i in range(self.k):
            period = period_list[i]

            # padding
            if (self.seq_len + self.pred_len) % period != 0:
                length = (((self.seq_len + self.pred_len) // period) + 1) * period
                padding = torch.zeros([B, length - (self.seq_len + self.pred_len), N]).to(x.device)
                out = torch.cat([x, padding], dim=1)
            else:
                length = (self.seq_len + self.pred_len)
                out = x

            # reshape: [B, T, N] -> [B, N, H, W]
            out = out.reshape(B, length // period, period, N).permute(0, 3, 1, 2).contiguous()
            # 2D conv
            out = self.conv(out)
            # reshape back: [B, N, H, W] -> [B, T, N]
            out = out.permute(0, 2, 3, 1).reshape(B, -1, N)
            res.append(out[:, :(self.seq_len + self.pred_len), :])

        # 3. Â†ÜÂè†ÊâÄÊúâÂë®ÊúüÁâπÂæÅÔºà‰∏•Ê†ºÂ§çÂàªÂéüÁâàËÅöÂêàÁöÑÂº†ÈáèÂΩ¢Áä∂Ôºâ
        # list[[B, L, N]] -> [B, K, L, N]
        res = torch.stack(res, dim=1)

        # 4. ‰ΩøÁî® FFT ÂπÖÂ∫¶‰Ωú‰∏∫ÊùÉÈáç (ÂéüÁâà TimesNet ÈÄªËæë)
        # period_weight: [B, K] -> Softmax ÂΩí‰∏ÄÂåñ -> [B, K, 1, 1]
        weights = F.softmax(period_weight, dim=1).unsqueeze(-1).unsqueeze(-1)

        # Ë∞ÉËØïÔºöÁ°ÆËÆ§ÂπøÊí≠ÂΩ¢Áä∂
        # È¢ÑÊúü weights: [B, K, 1, 1], res: [B, K, L, N]
        print("TimesBlockPro fusion shapes -> weights:", weights.shape, "res:", res.shape)

        # 5. Âä†ÊùÉËûçÂêàÔºàdim=1 ÊòØ K Áª¥Â∫¶Ôºâ: [B, K, L, N] -> [B, L, N]
        res = torch.sum(res * weights, dim=1)

        # 6. ÊÆãÂ∑ÆËøûÊé•
        res = res + x

        return res


class Model(nn.Module):
    """
    TimesNetPro: TimesNet with Adaptive Period Attention
    
    ‰ΩøÁî®ÊñπÊ≥ïÔºö
    ‰∏éÂéüÁâà TimesNet ÂÆåÂÖ®ÂÖºÂÆπÔºåÂèØ‰ª•Áõ¥Êé•ÊõøÊç¢‰ΩøÁî®
    """
    
    def __init__(self, configs):
        super(Model, self).__init__()
        self.configs = configs
        self.task_name = configs.task_name
        self.seq_len = configs.seq_len
        self.label_len = configs.label_len
        self.pred_len = configs.pred_len
        
        # ‰ΩøÁî®ÊîπËøõÁöÑ TimesBlockPro
        self.model = nn.ModuleList([TimesBlockPro(configs)
                                    for _ in range(configs.e_layers)])
        
        # Embedding Â±Ç
        from layers.Embed import DataEmbedding
        self.enc_embedding = DataEmbedding(configs.enc_in, configs.d_model,
                                          configs.embed, configs.freq,
                                          configs.dropout)
        
        self.layer = configs.e_layers
        self.layer_norm = nn.LayerNorm(configs.d_model)
        
        # Projection Â±Ç
        if self.task_name == 'imputation' or self.task_name == 'anomaly_detection':
            self.projection = nn.Linear(configs.d_model, configs.c_out, bias=True)
        if self.task_name == 'long_term_forecast' or self.task_name == 'short_term_forecast':
            self.predict_linear = nn.Linear(
                self.seq_len, self.pred_len + self.seq_len)
            self.projection = nn.Linear(
                configs.d_model, configs.c_out, bias=True)
        if self.task_name == 'classification':
            self.act = F.gelu
            self.dropout = nn.Dropout(configs.dropout)
            self.projection = nn.Linear(
                configs.d_model * configs.seq_len, configs.num_class)
    
    def forecast(self, x_enc, x_mark_enc, x_dec, x_mark_dec):
        # Normalization
        means = x_enc.mean(1, keepdim=True).detach()
        x_enc = x_enc.sub(means)
        stdev = torch.sqrt(
            torch.var(x_enc, dim=1, keepdim=True, unbiased=False) + 1e-5)
        x_enc = x_enc.div(stdev)
        
        # Embedding
        enc_out = self.enc_embedding(x_enc, x_mark_enc)
        enc_out = self.predict_linear(enc_out.permute(0, 2, 1)).permute(0, 2, 1)
        
        # TimesNetPro blocks
        for i in range(self.layer):
            enc_out = self.layer_norm(self.model[i](enc_out))
        
        # Projection
        dec_out = self.projection(enc_out)
        
        # De-normalization
        dec_out = dec_out.mul(
            (stdev[:, 0, :].unsqueeze(1).repeat(
                1, self.pred_len + self.seq_len, 1)))
        dec_out = dec_out.add(
            (means[:, 0, :].unsqueeze(1).repeat(
                1, self.pred_len + self.seq_len, 1)))
        return dec_out
    
    def imputation(self, x_enc, x_mark_enc, x_dec, x_mark_dec, mask):
        # Normalization
        means = torch.sum(x_enc, dim=1) / torch.sum(mask == 1, dim=1)
        means = means.unsqueeze(1).detach()
        x_enc = x_enc.sub(means)
        x_enc = x_enc.masked_fill(mask == 0, 0)
        stdev = torch.sqrt(torch.sum(x_enc * x_enc, dim=1) /
                           torch.sum(mask == 1, dim=1) + 1e-5)
        stdev = stdev.unsqueeze(1).detach()
        x_enc = x_enc.div(stdev)
        
        # Embedding
        enc_out = self.enc_embedding(x_enc, x_mark_enc)
        
        # TimesNetPro blocks
        for i in range(self.layer):
            enc_out = self.layer_norm(self.model[i](enc_out))
        
        # Projection
        dec_out = self.projection(enc_out)
        
        # De-normalization
        dec_out = dec_out.mul(
            (stdev[:, 0, :].unsqueeze(1).repeat(
                1, self.seq_len + self.pred_len, 1)))
        dec_out = dec_out.add(
            (means[:, 0, :].unsqueeze(1).repeat(
                1, self.seq_len + self.pred_len, 1)))
        return dec_out
    
    def anomaly_detection(self, x_enc):
        """
        ÂºÇÂ∏∏Ê£ÄÊµãÂâçÂêë‰º†Êí≠
        """
        # Normalization
        means = x_enc.mean(1, keepdim=True).detach()
        x_enc = x_enc.sub(means)
        stdev = torch.sqrt(
            torch.var(x_enc, dim=1, keepdim=True, unbiased=False) + 1e-5)
        x_enc = x_enc.div(stdev)
        
        # Embedding
        enc_out = self.enc_embedding(x_enc, None)
        
        # TimesNetPro blocks
        for i in range(self.layer):
            enc_out = self.layer_norm(self.model[i](enc_out))
        
        # Projection
        dec_out = self.projection(enc_out)
        
        # De-normalization
        dec_out = dec_out.mul(
            (stdev[:, 0, :].unsqueeze(1).repeat(
                1, self.pred_len + self.seq_len, 1)))
        dec_out = dec_out.add(
            (means[:, 0, :].unsqueeze(1).repeat(
                1, self.pred_len + self.seq_len, 1)))
        return dec_out
    
    def classification(self, x_enc, x_mark_enc):
        # Embedding
        enc_out = self.enc_embedding(x_enc, None)
        
        # TimesNetPro blocks
        for i in range(self.layer):
            enc_out = self.layer_norm(self.model[i](enc_out))
        
        # Output
        output = self.act(enc_out)
        output = self.dropout(output)
        output = output * x_mark_enc.unsqueeze(-1)
        output = output.reshape(output.shape[0], -1)
        output = self.projection(output)
        return output
    
    def forward(self, x_enc, x_mark_enc=None, x_dec=None, x_mark_dec=None, mask=None):
        if self.task_name == 'long_term_forecast' or self.task_name == 'short_term_forecast':
            dec_out = self.forecast(x_enc, x_mark_enc, x_dec, x_mark_dec)
            return dec_out[:, -self.pred_len:, :]
        if self.task_name == 'imputation':
            dec_out = self.imputation(x_enc, x_mark_enc, x_dec, x_mark_dec, mask)
            return dec_out
        if self.task_name == 'anomaly_detection':
            dec_out = self.anomaly_detection(x_enc)
            return dec_out
        if self.task_name == 'classification':
            dec_out = self.classification(x_enc, x_mark_enc)
            return dec_out
        return None
